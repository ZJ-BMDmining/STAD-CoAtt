{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca49ee4-da14-4aa2-9d00-a05669342c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14/May/2024 21:24:36] INFO - Note: NumExpr detected 16 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "[14/May/2024 21:24:36] INFO - NumExpr defaulting to 8 threads.\n",
      "TIME tensor(2) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yrc/STAD/taskers_utils.py:98: UserWarning: torch.sparse.SparseTensor(indices, values, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:593.)\n",
      "  out = torch.sparse.FloatTensor(idx.t(),vals).coalesce()\n",
      "/home/yrc/STAD/utils.py:69: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
      "  return torch.sparse.LongTensor(adj['idx'].t(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits sizes:  train 2\n"
     ]
    }
   ],
   "source": [
    "import utils as u\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import sbm_dl as sbm\n",
    "import link_pred_tasker as lpt\n",
    "import models as mls\n",
    "import egcn_o\n",
    "import splitter as sp\n",
    "import Cross_Entropy as ce\n",
    "import trainer as tr\n",
    "\n",
    "args = u.Namespace({\n",
    "    \"num_hist_steps\": 0, \n",
    "    \"adj_mat_time_window\": 1,\n",
    "    \"smart_neg_sampling\": True, \n",
    "    \"negative_mult_training\": 50,\n",
    "    \"negative_mult_test\": 100,\n",
    "    'use_2_hot_node_feats': False,\n",
    "    'use_1_hot_node_feats': True,\n",
    "    'aggr_time': 1,\n",
    "    'train_proportion': 1,\n",
    "    'dev_proportion': 0,\n",
    "    'task':'link_pred'\n",
    "})  \n",
    "\n",
    "args.data_loading_params = {'batch_size': 1,'num_workers': 8}\n",
    "args.sbm_args = {'folder': './data/',  'edges_file': 'GEO.csv',  'aggr_time': 1 ,'feats_per_node': 3}\n",
    "args.gcn_parameters = {'feats_per_node': 100,  'feats_per_node_min': 50,  'feats_per_node_max': 256,  'layer_1_feats': 100,\n",
    "                       'layer_1_feats_min': 10,  'layer_1_feats_max': 200,  'layer_2_feats': 100,  'layer_2_feats_same_as_l1': True,\n",
    "                       'k_top_grcu': 200,  'num_layers': 2,  'lstm_l1_layers': 1,  'lstm_l1_feats': 100, 'lstm_l1_feats_min': 10,\n",
    "                       'lstm_l1_feats_max': 200,  'lstm_l2_layers': 1,  'lstm_l2_feats': None, 'lstm_l2_feats_same_as_l1': True,\n",
    "                       'cls_feats': 100, 'cls_feats_min': 64,  'cls_feats_max': 800}\n",
    "\n",
    "dataset = sbm.sbm_dataset(args)\n",
    "tasker = lpt.Link_Pred_Tasker(args,dataset)\n",
    "splitter = sp.splitter(args,tasker)\n",
    "gcn_args = u.Namespace(args.gcn_parameters)\n",
    "gcn_args.feats_per_node = tasker.feats_per_node\n",
    "gcn = egcn_o.EGCN(gcn_args, activation = torch.nn.RReLU())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af94cccf-946c-495f-85c1-d6842b7854e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahat shape: torch.Size([5592, 5592])\n",
      "node_embs shape: torch.Size([5592, 5351])\n",
      "GCN_weights shape: torch.Size([5351, 100])\n",
      "Ahat shape: torch.Size([5592, 5592])\n",
      "node_embs shape: torch.Size([5592, 100])\n",
      "GCN_weights shape: torch.Size([100, 100])\n",
      "tensor([[ 0.0056,  0.0051, -0.0016,  ...,  0.0143,  0.0051, -0.0013],\n",
      "        [ 0.0053,  0.0049, -0.0012,  ...,  0.0143,  0.0033, -0.0010],\n",
      "        [ 0.0048,  0.0042, -0.0019,  ...,  0.0137,  0.0036, -0.0007],\n",
      "        ...,\n",
      "        [ 0.0051,  0.0046, -0.0024,  ...,  0.0137,  0.0043, -0.0006],\n",
      "        [ 0.0051,  0.0053, -0.0019,  ...,  0.0133,  0.0048, -0.0012],\n",
      "        [ 0.0056,  0.0049, -0.0021,  ...,  0.0135,  0.0047, -0.0006]],\n",
      "       grad_fn=<RreluWithNoiseBackward0>)\n",
      "Ahat shape: torch.Size([5592, 5592])\n",
      "node_embs shape: torch.Size([5592, 5351])\n",
      "GCN_weights shape: torch.Size([5351, 100])\n",
      "Ahat shape: torch.Size([5592, 5592])\n",
      "node_embs shape: torch.Size([5592, 100])\n",
      "GCN_weights shape: torch.Size([100, 100])\n",
      "tensor([[ 0.0060,  0.0049, -0.0014,  ...,  0.0134,  0.0053, -0.0008],\n",
      "        [ 0.0055,  0.0046, -0.0032,  ...,  0.0135,  0.0046, -0.0012],\n",
      "        [ 0.0054,  0.0043, -0.0030,  ...,  0.0135,  0.0054, -0.0014],\n",
      "        ...,\n",
      "        [ 0.0046,  0.0043, -0.0031,  ...,  0.0135,  0.0055, -0.0007],\n",
      "        [ 0.0053,  0.0044, -0.0020,  ...,  0.0137,  0.0048, -0.0007],\n",
      "        [ 0.0055,  0.0040, -0.0029,  ...,  0.0141,  0.0044, -0.0011]],\n",
      "       grad_fn=<RreluWithNoiseBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0056,  0.0051, -0.0016,  ...,  0.0143,  0.0051, -0.0013],\n",
       "         [ 0.0053,  0.0049, -0.0012,  ...,  0.0143,  0.0033, -0.0010],\n",
       "         [ 0.0048,  0.0042, -0.0019,  ...,  0.0137,  0.0036, -0.0007],\n",
       "         ...,\n",
       "         [ 0.0051,  0.0046, -0.0024,  ...,  0.0137,  0.0043, -0.0006],\n",
       "         [ 0.0051,  0.0053, -0.0019,  ...,  0.0133,  0.0048, -0.0012],\n",
       "         [ 0.0056,  0.0049, -0.0021,  ...,  0.0135,  0.0047, -0.0006]],\n",
       "        grad_fn=<RreluWithNoiseBackward0>),\n",
       " tensor([[ 0.0060,  0.0049, -0.0014,  ...,  0.0134,  0.0053, -0.0008],\n",
       "         [ 0.0055,  0.0046, -0.0032,  ...,  0.0135,  0.0046, -0.0012],\n",
       "         [ 0.0054,  0.0043, -0.0030,  ...,  0.0135,  0.0054, -0.0014],\n",
       "         ...,\n",
       "         [ 0.0046,  0.0043, -0.0031,  ...,  0.0135,  0.0055, -0.0007],\n",
       "         [ 0.0053,  0.0044, -0.0020,  ...,  0.0137,  0.0048, -0.0007],\n",
       "         [ 0.0055,  0.0040, -0.0029,  ...,  0.0141,  0.0044, -0.0011]],\n",
       "        grad_fn=<RreluWithNoiseBackward0>)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_nodes = dataset.num_nodes\n",
    "def ignore_batch_dim(adj):\n",
    "    adj['idx'] = adj['idx'][0]\n",
    "    adj['vals'] = adj['vals'][0]\n",
    "    return adj\n",
    "def prepare_sample(sample):\n",
    "    sample = u.Namespace(sample)\n",
    "    for i,adj in enumerate(sample.hist_adj_list):\n",
    "        adj = u.sparse_prepare_tensor(adj,torch_size = [num_nodes])\n",
    "        sample.hist_adj_list[i] = adj\n",
    "        \n",
    "        nodes = splitter.tasker.prepare_node_feats(sample.hist_ndFeats_list[i])\n",
    "        \n",
    "        sample.hist_ndFeats_list[i] = nodes\n",
    "        node_mask = sample.node_mask_list[i]\n",
    "        sample.node_mask_list[i] = node_mask.t()\n",
    "        \n",
    "    label_sp = ignore_batch_dim(sample.label_sp)\n",
    "    \n",
    "    label_sp['idx'] = label_sp['idx'].t()\n",
    "    \n",
    "    label_sp['vals'] = label_sp['vals'].type(torch.long)\n",
    "    sample.label_sp = label_sp\n",
    "    \n",
    "    return sample\n",
    "\n",
    "spatial = []\n",
    "\n",
    "for s in splitter.train:\n",
    "    s = prepare_sample(s)\n",
    "    nodes_embs = gcn(s.hist_adj_list,\n",
    "                     s.hist_ndFeats_list,\n",
    "                     s.node_mask_list)\n",
    "    print(nodes_embs)\n",
    "    spatial.append(nodes_embs)\n",
    "spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f55724a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "for i in range(len(spatial)):\n",
    "    reshaped_tensor = torch.squeeze(spatial[i]).view(5592, 100)\n",
    "    numpy_array = reshaped_tensor.detach().numpy()\n",
    "    final = pd.DataFrame(numpy_array)\n",
    "    final.to_csv(\"stage_\"+str(i)+\"_GEO.csv\",index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ace3131f-f9c8-4406-b976-451793b002e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, size, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.a_2 = nn.Parameter(torch.ones(size))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size, mid_size, out_size, dropout_r=0., use_relu=True):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(in_size, mid_size,dtype=torch.float),\n",
    "            nn.ReLU(inplace=True) if use_relu else nn.Identity(),\n",
    "            nn.Dropout(dropout_r) if dropout_r > 0 else nn.Identity(),\n",
    "            nn.Linear(mid_size, out_size,dtype=torch.float)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "class MHAtt(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(MHAtt, self).__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.linear_v = nn.Linear(config.Feature1_SIZE, config.HIDDEN_SIZE,dtype=torch.float)\n",
    "        self.linear_k = nn.Linear(config.Feature2_SIZE, config.HIDDEN_SIZE,dtype=torch.float)\n",
    "        self.linear_q = nn.Linear(config.Feature1_SIZE, config.HIDDEN_SIZE,dtype=torch.float)\n",
    "        self.linear_merge = nn.Linear(config.HIDDEN_SIZE, config.Feature1_SIZE,dtype=torch.float)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.DROPOUT_R)\n",
    "\n",
    "    def forward(self, v, k, q, mask):\n",
    "        n_batches = q.size(0)\n",
    "\n",
    "        v = self.linear_v(v).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.config.MULTI_HEAD,\n",
    "            self.config.HIDDEN_SIZE_HEAD\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        k = self.linear_k(k).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.config.MULTI_HEAD,\n",
    "            self.config.HIDDEN_SIZE_HEAD\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        q = self.linear_q(q).view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.config.MULTI_HEAD,\n",
    "            self.config.HIDDEN_SIZE_HEAD\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        atted = self.att(v, k, q, mask)\n",
    "        atted = atted.transpose(1, 2).contiguous().view(\n",
    "            n_batches,\n",
    "            -1,\n",
    "            self.config.HIDDEN_SIZE\n",
    "        )\n",
    "        atted = self.linear_merge(atted)\n",
    "\n",
    "        return atted\n",
    "\n",
    "    def att(self, value, key, query, mask):\n",
    "        d_k = query.size(-1)\n",
    "\n",
    "        scores = torch.matmul(\n",
    "            query, key.transpose(-2, -1)\n",
    "        ) / math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "        att_map = F.softmax(scores, dim=-1)\n",
    "        att_map = self.dropout(att_map)\n",
    "\n",
    "        return torch.matmul(att_map, value)\n",
    "\n",
    "class SGA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SGA, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MHAtt(config).float()\n",
    "        # self.multi_head_attention2 = MHAtt(config).float()\n",
    "        self.feed_forward = MLP(config.Feature1_SIZE, config.FF_SIZE, config.Feature1_SIZE)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(config.DROPOUT_R)\n",
    "        self.layer_norm1 = LayerNorm(config.Feature1_SIZE)\n",
    "\n",
    "        # self.dropout2 = nn.Dropout(config.DROPOUT_R)\n",
    "        # self.layer_norm2 = LayerNorm(config.Feature1_SIZE)\n",
    "\n",
    "        self.dropout3 = nn.Dropout(config.DROPOUT_R)\n",
    "        self.layer_norm3 = LayerNorm(config.Feature1_SIZE)\n",
    "\n",
    "    def forward(self, x, y, x_mask, y_mask):\n",
    "        x = self.layer_norm1(x + self.dropout1(\n",
    "            self.multi_head_attention1(x, y, x, x_mask)\n",
    "        ))\n",
    "        # x = self.layer_norm2(x + self.dropout2(\n",
    "        #     self.multi_head_attention2(x, y, x, y_mask)\n",
    "        # ))\n",
    "        x = self.layer_norm3(x + self.dropout3(\n",
    "            self.feed_forward(x)\n",
    "        ))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0134162-36c8-4699-bdb1-94b5c3cbb3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure parameters\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.HIDDEN_SIZE = 512\n",
    "        self.FF_SIZE = 512\n",
    "        self.DROPOUT_R = 0.1\n",
    "        self.MULTI_HEAD = 8\n",
    "        assert self.HIDDEN_SIZE % self.MULTI_HEAD == 0\n",
    "        self.HIDDEN_SIZE_HEAD = int(self.HIDDEN_SIZE / self.MULTI_HEAD)\n",
    "        self.Feature1_SIZE = spatial[0].shape[1]\n",
    "        self.Feature2_SIZE = spatial[0].shape[1]\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "\n",
    "# Initialize SGA module\n",
    "sga_module = SGA(config)\n",
    "\n",
    "# Generate masks\n",
    "def make_mask(feature):\n",
    "    return (torch.sum(torch.abs(feature), dim=-1) == 0).unsqueeze(1).unsqueeze(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ddeef63-97e8-42d7-be0a-0f551cd2edae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.8538, -0.6988,  0.7390,  ...,  1.3109, -0.4787,  1.0033],\n",
      "         [-0.9348, -0.5899,  0.7842,  ...,  1.4919, -0.3146,  1.1863],\n",
      "         [-0.0964, -0.7644, -0.2824,  ...,  1.4550, -0.4800,  0.9489],\n",
      "         ...,\n",
      "         [-0.8703, -0.8020,  0.5409,  ...,  1.3173, -0.0541,  1.0655],\n",
      "         [-1.1832, -0.9378,  0.5809,  ...,  1.5389, -0.4571,  1.1874],\n",
      "         [-0.9058, -0.7987,  0.5589,  ...,  1.5868, -0.3842,  1.1088]]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.8538, -0.6988,  0.7390,  ...,  1.3109, -0.4787,  1.0033],\n",
       "         [-0.9348, -0.5899,  0.7842,  ...,  1.4919, -0.3146,  1.1863],\n",
       "         [-0.0964, -0.7644, -0.2824,  ...,  1.4550, -0.4800,  0.9489],\n",
       "         ...,\n",
       "         [-0.8703, -0.8020,  0.5409,  ...,  1.3173, -0.0541,  1.0655],\n",
       "         [-1.1832, -0.9378,  0.5809,  ...,  1.5389, -0.4571,  1.1874],\n",
       "         [-0.9058, -0.7987,  0.5589,  ...,  1.5868, -0.3842,  1.1088]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "with torch.no_grad():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    X1 = spatial[0].unsqueeze(0).float().to('cuda')\n",
    "    X2 = spatial[1].unsqueeze(0).float().to('cuda')\n",
    "    x_mask = make_mask(X1.float()).to('cuda')\n",
    "    y_mask = make_mask(X2.float()).to('cuda')\n",
    "    sga_module.to('cuda')\n",
    "    current_input = sga_module(X1, X2, x_mask, y_mask)\n",
    "    print(current_input)\n",
    "    for i in range(2,len(spatial)):\n",
    "        # del X1,X2,x_mask,y_mask\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        X1 = current_input\n",
    "        X2 = spatial[i].unsqueeze(0).float().to('cuda')\n",
    "        x_mask = make_mask(X1.float()).to('cuda')\n",
    "        y_mask = make_mask(X2.float()).to('cuda')\n",
    "        current_input = sga_module(X1, X2, x_mask, y_mask)\n",
    "        print(current_input)\n",
    "current_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9d755eb-9728-4473-bc9a-eddb7c9ab2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5592, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab80a9e9-010e-40a9-83da-27ecb2d25f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.853835</td>\n",
       "      <td>-0.698774</td>\n",
       "      <td>0.738998</td>\n",
       "      <td>-0.045378</td>\n",
       "      <td>-0.116367</td>\n",
       "      <td>0.174600</td>\n",
       "      <td>-0.243816</td>\n",
       "      <td>2.487399</td>\n",
       "      <td>0.558009</td>\n",
       "      <td>-0.727456</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016880</td>\n",
       "      <td>0.731542</td>\n",
       "      <td>0.991435</td>\n",
       "      <td>0.381862</td>\n",
       "      <td>0.668487</td>\n",
       "      <td>0.585468</td>\n",
       "      <td>0.179599</td>\n",
       "      <td>1.310859</td>\n",
       "      <td>-0.478688</td>\n",
       "      <td>1.003326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.934829</td>\n",
       "      <td>-0.589923</td>\n",
       "      <td>0.784242</td>\n",
       "      <td>0.264519</td>\n",
       "      <td>-0.170699</td>\n",
       "      <td>0.268877</td>\n",
       "      <td>-0.120890</td>\n",
       "      <td>0.523225</td>\n",
       "      <td>0.630536</td>\n",
       "      <td>-0.516873</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>0.813827</td>\n",
       "      <td>0.965184</td>\n",
       "      <td>0.651837</td>\n",
       "      <td>0.838446</td>\n",
       "      <td>0.639399</td>\n",
       "      <td>0.256180</td>\n",
       "      <td>1.491901</td>\n",
       "      <td>-0.314560</td>\n",
       "      <td>1.186334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.096383</td>\n",
       "      <td>-0.764445</td>\n",
       "      <td>-0.282430</td>\n",
       "      <td>-0.037582</td>\n",
       "      <td>-0.120513</td>\n",
       "      <td>0.310358</td>\n",
       "      <td>-0.198896</td>\n",
       "      <td>2.437239</td>\n",
       "      <td>0.627342</td>\n",
       "      <td>-0.698358</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346342</td>\n",
       "      <td>0.771247</td>\n",
       "      <td>1.014533</td>\n",
       "      <td>0.381136</td>\n",
       "      <td>0.739023</td>\n",
       "      <td>0.543372</td>\n",
       "      <td>0.222055</td>\n",
       "      <td>1.454999</td>\n",
       "      <td>-0.480026</td>\n",
       "      <td>0.948938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.866893</td>\n",
       "      <td>-0.325123</td>\n",
       "      <td>0.549122</td>\n",
       "      <td>0.118293</td>\n",
       "      <td>-0.192852</td>\n",
       "      <td>-0.129002</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>2.382843</td>\n",
       "      <td>0.510228</td>\n",
       "      <td>-0.468551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042357</td>\n",
       "      <td>0.696352</td>\n",
       "      <td>0.991708</td>\n",
       "      <td>0.445259</td>\n",
       "      <td>-0.506932</td>\n",
       "      <td>0.471720</td>\n",
       "      <td>0.400520</td>\n",
       "      <td>1.545528</td>\n",
       "      <td>-0.460037</td>\n",
       "      <td>1.163922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.937585</td>\n",
       "      <td>-0.714162</td>\n",
       "      <td>0.638218</td>\n",
       "      <td>0.559154</td>\n",
       "      <td>-0.285168</td>\n",
       "      <td>0.150995</td>\n",
       "      <td>-0.559380</td>\n",
       "      <td>0.436686</td>\n",
       "      <td>0.510724</td>\n",
       "      <td>-0.669533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285589</td>\n",
       "      <td>0.623705</td>\n",
       "      <td>0.910474</td>\n",
       "      <td>0.604349</td>\n",
       "      <td>0.612439</td>\n",
       "      <td>0.528975</td>\n",
       "      <td>0.317026</td>\n",
       "      <td>1.504469</td>\n",
       "      <td>-0.524921</td>\n",
       "      <td>1.272971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5587</th>\n",
       "      <td>0.079247</td>\n",
       "      <td>0.216752</td>\n",
       "      <td>-1.184817</td>\n",
       "      <td>-0.284532</td>\n",
       "      <td>-1.173024</td>\n",
       "      <td>-0.463248</td>\n",
       "      <td>-1.385232</td>\n",
       "      <td>1.532151</td>\n",
       "      <td>2.073771</td>\n",
       "      <td>-0.774980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.571327</td>\n",
       "      <td>1.279978</td>\n",
       "      <td>-0.785562</td>\n",
       "      <td>-0.741090</td>\n",
       "      <td>-1.308580</td>\n",
       "      <td>-0.615403</td>\n",
       "      <td>2.086149</td>\n",
       "      <td>1.802725</td>\n",
       "      <td>-0.201333</td>\n",
       "      <td>-0.485090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5588</th>\n",
       "      <td>-0.980605</td>\n",
       "      <td>-0.892185</td>\n",
       "      <td>0.553306</td>\n",
       "      <td>0.458857</td>\n",
       "      <td>-0.152461</td>\n",
       "      <td>0.172069</td>\n",
       "      <td>-0.105800</td>\n",
       "      <td>2.463047</td>\n",
       "      <td>0.318549</td>\n",
       "      <td>-0.713937</td>\n",
       "      <td>...</td>\n",
       "      <td>0.125182</td>\n",
       "      <td>0.817362</td>\n",
       "      <td>0.993720</td>\n",
       "      <td>0.550011</td>\n",
       "      <td>0.759665</td>\n",
       "      <td>0.535267</td>\n",
       "      <td>0.370594</td>\n",
       "      <td>1.611545</td>\n",
       "      <td>-0.517124</td>\n",
       "      <td>1.205186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5589</th>\n",
       "      <td>-0.870327</td>\n",
       "      <td>-0.801968</td>\n",
       "      <td>0.540880</td>\n",
       "      <td>-0.024898</td>\n",
       "      <td>-0.126186</td>\n",
       "      <td>0.112049</td>\n",
       "      <td>-0.544033</td>\n",
       "      <td>2.415838</td>\n",
       "      <td>0.518048</td>\n",
       "      <td>-0.705705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.025727</td>\n",
       "      <td>0.652586</td>\n",
       "      <td>1.009885</td>\n",
       "      <td>0.526750</td>\n",
       "      <td>1.092181</td>\n",
       "      <td>0.715187</td>\n",
       "      <td>0.118085</td>\n",
       "      <td>1.317252</td>\n",
       "      <td>-0.054133</td>\n",
       "      <td>1.065456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5590</th>\n",
       "      <td>-1.183180</td>\n",
       "      <td>-0.937780</td>\n",
       "      <td>0.580935</td>\n",
       "      <td>-0.728721</td>\n",
       "      <td>-0.286498</td>\n",
       "      <td>0.166919</td>\n",
       "      <td>-0.546322</td>\n",
       "      <td>0.591584</td>\n",
       "      <td>0.009069</td>\n",
       "      <td>-0.489089</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240434</td>\n",
       "      <td>0.732498</td>\n",
       "      <td>1.003324</td>\n",
       "      <td>0.818964</td>\n",
       "      <td>0.811158</td>\n",
       "      <td>0.667322</td>\n",
       "      <td>0.160316</td>\n",
       "      <td>1.538945</td>\n",
       "      <td>-0.457087</td>\n",
       "      <td>1.187384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591</th>\n",
       "      <td>-0.905767</td>\n",
       "      <td>-0.798668</td>\n",
       "      <td>0.558907</td>\n",
       "      <td>0.052858</td>\n",
       "      <td>-0.087973</td>\n",
       "      <td>-0.132980</td>\n",
       "      <td>-0.114883</td>\n",
       "      <td>2.394054</td>\n",
       "      <td>0.532630</td>\n",
       "      <td>-0.655782</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004453</td>\n",
       "      <td>0.732120</td>\n",
       "      <td>1.055030</td>\n",
       "      <td>0.426829</td>\n",
       "      <td>0.720586</td>\n",
       "      <td>0.524347</td>\n",
       "      <td>0.177123</td>\n",
       "      <td>1.586841</td>\n",
       "      <td>-0.384190</td>\n",
       "      <td>1.108840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5592 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.853835 -0.698774  0.738998 -0.045378 -0.116367  0.174600 -0.243816   \n",
       "1    -0.934829 -0.589923  0.784242  0.264519 -0.170699  0.268877 -0.120890   \n",
       "2    -0.096383 -0.764445 -0.282430 -0.037582 -0.120513  0.310358 -0.198896   \n",
       "3    -0.866893 -0.325123  0.549122  0.118293 -0.192852 -0.129002  0.000151   \n",
       "4    -0.937585 -0.714162  0.638218  0.559154 -0.285168  0.150995 -0.559380   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5587  0.079247  0.216752 -1.184817 -0.284532 -1.173024 -0.463248 -1.385232   \n",
       "5588 -0.980605 -0.892185  0.553306  0.458857 -0.152461  0.172069 -0.105800   \n",
       "5589 -0.870327 -0.801968  0.540880 -0.024898 -0.126186  0.112049 -0.544033   \n",
       "5590 -1.183180 -0.937780  0.580935 -0.728721 -0.286498  0.166919 -0.546322   \n",
       "5591 -0.905767 -0.798668  0.558907  0.052858 -0.087973 -0.132980 -0.114883   \n",
       "\n",
       "            7         8         9   ...        90        91        92  \\\n",
       "0     2.487399  0.558009 -0.727456  ... -0.016880  0.731542  0.991435   \n",
       "1     0.523225  0.630536 -0.516873  ...  0.015461  0.813827  0.965184   \n",
       "2     2.437239  0.627342 -0.698358  ... -0.346342  0.771247  1.014533   \n",
       "3     2.382843  0.510228 -0.468551  ...  0.042357  0.696352  0.991708   \n",
       "4     0.436686  0.510724 -0.669533  ...  0.285589  0.623705  0.910474   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "5587  1.532151  2.073771 -0.774980  ... -0.571327  1.279978 -0.785562   \n",
       "5588  2.463047  0.318549 -0.713937  ...  0.125182  0.817362  0.993720   \n",
       "5589  2.415838  0.518048 -0.705705  ... -0.025727  0.652586  1.009885   \n",
       "5590  0.591584  0.009069 -0.489089  ... -0.240434  0.732498  1.003324   \n",
       "5591  2.394054  0.532630 -0.655782  ... -0.004453  0.732120  1.055030   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "0     0.381862  0.668487  0.585468  0.179599  1.310859 -0.478688  1.003326  \n",
       "1     0.651837  0.838446  0.639399  0.256180  1.491901 -0.314560  1.186334  \n",
       "2     0.381136  0.739023  0.543372  0.222055  1.454999 -0.480026  0.948938  \n",
       "3     0.445259 -0.506932  0.471720  0.400520  1.545528 -0.460037  1.163922  \n",
       "4     0.604349  0.612439  0.528975  0.317026  1.504469 -0.524921  1.272971  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "5587 -0.741090 -1.308580 -0.615403  2.086149  1.802725 -0.201333 -0.485090  \n",
       "5588  0.550011  0.759665  0.535267  0.370594  1.611545 -0.517124  1.205186  \n",
       "5589  0.526750  1.092181  0.715187  0.118085  1.317252 -0.054133  1.065456  \n",
       "5590  0.818964  0.811158  0.667322  0.160316  1.538945 -0.457087  1.187384  \n",
       "5591  0.426829  0.720586  0.524347  0.177123  1.586841 -0.384190  1.108840  \n",
       "\n",
       "[5592 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "reshaped_tensor = torch.squeeze(current_input).view(5592, 100)\n",
    "numpy_array = reshaped_tensor.cpu().numpy()\n",
    "final = pd.DataFrame(numpy_array)\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d955b25e-2465-48c6-b0b1-4e712c6b8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(\"final_spatial_fusion_GEO.csv\",index=None,columns=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c0c3a-0745-43e1-875e-e40c7d21bc0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
